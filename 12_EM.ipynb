{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM算法总结\n",
    "\n",
    "在概率模型中，最常用的模型参数估计方法应该就是**最大似然法**。\n",
    "\n",
    "**EM算法**本质上也是最大似然，它是针对模型中存在**隐变量**的情况的**最大似然**。\n",
    "\n",
    "下面通过两个例子引入。\n",
    "\n",
    "## 没有隐变量的硬币模型\n",
    "\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/maximum_likelihood.png)\n",
    "\n",
    "假设有两个硬币，$A$和$B$，这两个硬币具体材质未知，即抛硬币的结果是head的概率不一定是50%。\n",
    "\n",
    "在这个实验中，我们每次拿其中一个硬币，抛10次，统计结果。\n",
    "\n",
    "实验的目标是统计$A$和$B$的head朝上的概率，即估计$\\hat \\theta_A$和$\\hat \\theta_B$。\n",
    "\n",
    "对每一枚硬币来说，使用**极大似然法**来估计它的参数：\n",
    "\n",
    "假设硬币$A$正面朝上的次数是$n^A_h$，反面朝上的次数是：$n^A_t$。\n",
    "\n",
    "似然函数：$L(\\theta_A) = (\\theta_A)^{n^A_h}(1-\\theta_A)^{n^A_t}$。\n",
    "\n",
    "对数似然函数：$log\\;L(\\theta_A) = n^A_h\\cdot log(\\theta_A)+n^A_t\\cdot log(1-\\theta_A)$。\n",
    "\n",
    "$\\hat \\theta_A = \\underset{\\theta_A}{argmax}\\;log\\;L(\\theta_A)$ 。\n",
    "\n",
    "对参数求偏导：$\\frac{\\partial log\\; L(\\theta_A)}{\\partial \\theta_A}=\\frac{n^A_h}{\\theta_A}-\\frac{n^A_t}{1-\\theta_A}$。\n",
    "\n",
    "令上式为$0$，解得：$\\hat \\theta_A = \\frac{n^A_h}{n^A_h+n^A_t}$。\n",
    "\n",
    "即$\\hat \\theta_A = \\frac{number\\; of\\; heads\\; using\\; coin\\; A}{total\\; number\\; of\\; flips\\; using\\; coin\\; A}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有隐变量的硬币模型\n",
    "\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/expectation_maximization.png)\n",
    "\n",
    "这个问题是上一个问题的困难版，即给出一系列统计的实验，**但不告诉你某组实验采用的是哪枚硬币**，即某组实验采用哪枚硬币成了一个**隐变量**。\n",
    "\n",
    "这里引入**EM算法的思路**：\n",
    "\n",
    "- 1.先随机给出模型参数的估计，以初始化模型参数。\n",
    "- 2.根据之前模型参数的估计，和观测数据，计算**隐变量的分布**。\n",
    "- 3.根据隐变量的分布，求**联合分布的对数**关于隐变量分布的**期望**。\n",
    "- 4.重新估计**模型参数**，这次最大化的不是似然函数，而是第3步求的**期望**。\n",
    "\n",
    "一般教科书会把EM算法分成两步：E步和M步，即求期望和最大化期望。\n",
    "\n",
    "E步对应上面2,3；M对应4。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM算法\n",
    "\n",
    "输入：观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z|\\theta)$，条件分布$P(Z|Y,\\theta)$;\n",
    "\n",
    "输出：模型参数$\\theta$。\n",
    "\n",
    "- 1.选择参数的初始值$\\theta^{(0)}$，开始迭代；\n",
    "- 在第$i+1$次迭代:\n",
    "    - 2.E步：$Q(\\theta,\\theta^{(i)}) = \\sum_zlog\\;P(Y,Z|\\theta)P(Z|Y,\\theta^{(i)})$\n",
    "    - 3.M步：$Q^{(i+1)} = \\underset{\\theta}{argmax}\\;Q(\\theta,\\theta^{(i)})$\n",
    "- 4.重复2，3直至收敛。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- [如何感性地理解EM算法？](http://www.jianshu.com/p/1121509ac1dc)\n",
    "- [What is the expectation maximization algorithm?](http://pan.baidu.com/s/1i4NfvP7)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
