{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA主题模型学习总结\n",
    "\n",
    "`本篇博客是《LDA漫游指南》和《LDA数学八卦》的学习笔记。`\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [简介](#简介)\n",
    "    - [LDA算法输入与输出](#LDA算法输入与输出)\n",
    "- [前置知识](#前置知识)\n",
    "    - [gamma函数](#gamma函数)\n",
    "    - [二项分布](#二项分布)\n",
    "    - [Beta分布](#Beta分布)\n",
    "    - [多项分布](#多项分布)\n",
    "    - [Dirichlet分布](#Dirichlet分布)\n",
    "    - [共轭先验分布](#共轭先验分布)\n",
    "    - [MCMC](#MCMC)\n",
    "- [LDA推导](#LDA推导)\n",
    "    - [贝叶斯unigram](#贝叶斯unigram)\n",
    "    - [LDA模型的标准生成过程](#LDA模型的标准生成过程)\n",
    "    - [数学表示](#数学表示)\n",
    "- [交给Gibbs Sampling](#交给Gibbs-Sampling)\n",
    "    - [最终的Gibbs Smapling公式](#最终的Gibbs-Smapling公式)\n",
    "- [LDA训练](#LDA训练)\n",
    "- [LDA的inference](#LDA的inference)\n",
    "- [LDA实现](#LDA实现)\n",
    "    \n",
    "\n",
    "## 简介\n",
    "\n",
    "LDA（Latent Dirichlet Allocation）是一种**非监督**机器学习技术，可以用来识别大规模文档集或语料库中潜在隐藏的主题信息。\n",
    "\n",
    "LDA假设每个词是由背后的一个潜在隐藏的主题中抽取出来的，对于每篇文档，生成过程如下：\n",
    "- 1.对于每篇文档，从主题分布中抽取一个主题。\n",
    "- 2.从上述被抽到的主题所对应的单词分布中抽取一个单词。\n",
    "- 3.重复上述过程直到遍历文档中的每个单词。\n",
    "\n",
    "### LDA算法输入与输出\n",
    "- 输入：分词后的文章集。主题数$K$，超参数：$\\alpha$和$\\beta$。\n",
    "- 输出：\n",
    "    - 1.每篇文章每个词被指定的主题编号。\n",
    "    - 2.每篇文章的主题概率分布：$\\theta$\n",
    "    - 3.每个主题下的词概率分布：$\\phi$\n",
    "    - 4.词和id的映射表。\n",
    "    - 5.每个主题$\\phi$下\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置知识\n",
    "\n",
    "### gamma函数\n",
    "\n",
    "所谓的gamma函数就是阶乘的函数形式。\n",
    "\n",
    "$$\\Gamma(x)=\\int_0^{+\\infty}e^{-t}t^{x-1}dt\\;\\;\\;(x>0)$$\n",
    "\n",
    "$$\\Gamma(n) = (n-1)!$$\n",
    "\n",
    "### 二项分布\n",
    "\n",
    "打靶，$n$次中中了$k$次的概率：\n",
    "\n",
    "$$f(k;n,p)=Pr(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}$$\n",
    "\n",
    "### Beta分布\n",
    "\n",
    "$X\\sim Beta(\\alpha, \\beta)$\n",
    "\n",
    "概率密度函数：$$f(x;\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\\\=\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
    "\n",
    "期望：$$E(p) = \\int_0^1t\\cdot Beta(t|\\alpha, \\beta)dt\\\\=\\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "\n",
    "### 多项分布\n",
    "\n",
    "多项分布是二项分布的推广：投$n$次骰子，共有六种结果，概率为$p_i$，$i$点出现$x_i$次的组合概率：\n",
    "\n",
    "$$f(x_1, ...x_k;n,p_1,...,p_k)=Pr(X_1=x_1\\; and\\; ... and\\; X_k=x_k)\\\\=\\frac{n!}{x_1!...x_k!}p_1^{x_1}...p_k^{x_k}\\;\\;\\;when\\;\\sum_{i=1}^kx_i = n$$\n",
    "\n",
    "### Dirichlet分布\n",
    "\n",
    "$$p\\sim D(t|\\alpha)$$\n",
    "\n",
    "概率密度函数：$$f(p_1,..., p_k-1)=\\frac{1}{\\Delta (\\alpha)}\\prod_{i=1}^kp_i^{\\alpha_i-1}$$\n",
    "\n",
    "期望：$$E(p) = (\\frac{\\alpha_1}{\\sum_{i=1}^K\\alpha_i}, \\frac{\\alpha_2}{\\sum_{i=1}^K\\alpha_i}, ..., \\frac{\\alpha_K}{\\sum_{i=1}^K\\alpha_i})$$\n",
    "\n",
    "### 共轭先验分布\n",
    "\n",
    "贝叶斯公式：$$p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}$$\n",
    "\n",
    "即：**后验分布=似然函数×先验分布**\n",
    "\n",
    "**共轭**：选取一个函数作为似然函数，使得先验分布函数和后验分布函数的形式一致。\n",
    "\n",
    "- beta分布是二项分布的共轭先验分布，即，二项分布作为似然函数，先验分布是beta分布，后验分布依然是beta分布。\n",
    "- Dirichlet分布是多项式分布的共轭先验分布，即，多项式布作为似然函数，先验分布是Dirichlet分布，后验分布依然是Dirichlet分布。\n",
    "\n",
    "### MCMC\n",
    "\n",
    "参考之前的博客：https://applenob.github.io/1_MCMC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA推导\n",
    "\n",
    "### 贝叶斯unigram\n",
    "\n",
    "不考虑单词简单顺序，被称为“词袋模型”。\n",
    "\n",
    "$$P(W) = p(w_1)p(w_2)...p(w_n) = \\prod^V_{t=1}p_t^{n_t}\\;\\;\\;\\sum^V_{t=1}p_t = 1$$\n",
    "\n",
    "为什么似然是多项式分布？想象一个巨大的骰子，有$V$个面，每面代表一个词，每个面的概率是$\\vec{p}=(p_1, ...p_V)$，产生次数是：$\\vec{n} = (n_1, ..., n_V)$，那么生成某篇文章的概率是服从多项式分布的。\n",
    "\n",
    "贝叶斯学派认为参数也服从某种分布，即，不知道上帝用哪个骰子来生成文档，这个选取骰子的概率，服从Dirichlet分布。\n",
    "\n",
    "又有：$Dir(\\vec{p}|\\vec{\\alpha}) + MultCount(\\vec{n}) = Dir(\\vec{p}|\\vec{\\alpha}+\\vec{n})$，综合上面Dirichlet分布的期望，可以得到对于每一个$p_i$，可以如下**估计**：$\\tilde p_i = \\frac{n_i+\\alpha_i}{\\sum_{i=1}^V(n_i + \\alpha_i)}$。即，每个参数的估计值是其对应事件的先验的伪计数和数据中的计数的和在整体技术中的比例。\n",
    "\n",
    "进一步，计算出**文本语料的产生概率**是：$p(W|\\vec{\\alpha}) = \\int p(W|\\vec{p})p(\\vec{p}|\\vec{\\alpha})d\\vec{p}=\\frac{\\Delta(\\vec{n}+\\vec{\\alpha})}{\\Delta \\vec{\\alpha}}$\n",
    "\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/bayes_unigram.png)\n",
    "\n",
    "用通俗的话说，这是上帝从一个坛子中抽一个骰子，再丢这个骰子，观察结果的过程。\n",
    "\n",
    "### LDA模型的标准生成过程\n",
    "\n",
    "LDA相当于两个上面的步骤的结合（两个坛子）。上帝有两个大坛子，第一个坛子装doc-topic骰子，第二个坛子装topic-word骰子：\n",
    "\n",
    "- 1.选择$\\theta_i \\sim Dir(\\vec{\\alpha})$，这里$i\\in\\{1,2,...,M\\}$，$M$代表文章数。每生成一篇文章，从第一个坛子中选一个doc-topic骰子。\n",
    "- 2.选择$\\phi_i \\sim Dir(\\vec{\\beta})$，这里$k \\in \\{1,2,...,K\\}$，$K$代表主题个数。独立地挑了$K$个topic-word骰子。\n",
    "- 3.对每个单词的位置$W_{i,j}$，这里$j \\in \\{1,...,N_i\\}$，$i \\in \\{1,...,M\\}$\n",
    "    - 4.选择一个topic主题：$z_{i,j} \\sim Multinominal(\\theta_i)$。投掷这个doc-topic骰子，得到一个topic编号$z$。\n",
    "    - 5.选择一个word词：$w_{i,j} \\sim Multinominal(\\phi_{z_{i,j}})$。投掷topic是$z$的topic-word骰子，得到一个词。\n",
    "    \n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/lda.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 数学表示\n",
    "\n",
    "对每个doc-topic骰子，有：$p(\\vec z_m | \\vec \\alpha) = \\frac{\\Delta(\\vec n_m +\\vec  \\alpha)}{\\Delta \\vec \\alpha}$\n",
    "\n",
    "其中：$\\vec n_m = (n_m^{(1)}, .., n_m^{(K)})$，$n_m^{(k)}$代表第$m$篇文档中第$k$个topic产生的词的个数。$\\vec \\alpha$是$K$维的向量。\n",
    "\n",
    "因为M篇文章生成topic的过程上**相互独立**的，有M个doc-topic骰子的联合概率分布，即**整个语料的topics生成概率**：$$p(\\vec z| \\vec \\alpha) = \\prod^M_{m=1}p(\\vec z_m|\\vec \\alpha)\\\\=\\prod^M_{m=1}\\frac{\\Delta(\\vec n_m + \\vec \\alpha)}{\\Delta \\vec \\alpha}$$\n",
    "\n",
    "对每topic-word骰子，有：$p(\\vec w_k|\\vec \\beta) = \\frac{\\Delta(\\vec n_k + \\vec \\beta)}{\\Delta \\vec \\beta}$\n",
    "\n",
    "其中：$\\vec n_k = (n_k^{(1)}, .., n_k^{(V)})$，$n_k^{(v)}$代表第$k$个topic产生的词中，第$v$个word产生的词的个数。$\\vec \\beta\n",
    "$是$V$维的向量。\n",
    "\n",
    "因为K个topic生成word的过程也是**相互独立**的，有K个topic-word骰子的联合概率分布，即**整个语料中words生成的概率**：$$p(\\vec w|\\vec z, \\vec \\beta)\\\\=\\prod_{k=1}^Kp(\\vec w_{(k)}|\\vec z_{(k)}, \\vec \\beta)\\\\=\\prod_{k=1}^K\\frac{\\Delta(\\vec n_k + \\vec \\beta)}{\\Delta \\vec \\beta}$$\n",
    "\n",
    "联合上面两个联合概率分布，得到**整个语料中words生成的概率**和**整个语料的topics生成概率**的**联合概率分布**：\n",
    "\n",
    "$$p(\\vec w, \\vec z| \\vec \\alpha, \\vec \\beta)\\\\=p(\\vec w|\\vec z, \\vec \\beta)p(\\vec z| \\vec \\alpha)\\\\=\\prod_{k=1}^K\\frac{\\Delta(\\vec n_k + \\vec \\beta)}{\\Delta \\vec \\beta}\\prod^M_{m=1}\\frac{\\Delta(\\vec n_m + \\vec \\alpha)}{\\Delta \\vec \\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 交给Gibbs Sampling\n",
    "\n",
    "Gibbs Smapling建议先回顾下之前的[博客文章](https://applenob.github.io/1_MCMC.html#Gibbs-Sampling)。\n",
    "\n",
    "有了联合分布$p(\\vec w, \\vec z)$，可以使用Gibbs Sampling了。\n",
    "\n",
    "我们的**终极目标**是：要使用一个马尔科夫链，sample出一些列的状态点，使得最终的平稳分布状态就是我们给定的联合概率分布。\n",
    "\n",
    "语料库中第$i$个词对应的topic记为$z_i$，其中$i=(m,n)$是一个二维下标，对应第$m$篇文档的第$n$个词。$-i$表示去除下标$i$的词。\n",
    "\n",
    "我们要采样的分布是$p(\\vec z| \\vec w)$，根据Gibbs Sampling的要求，我们要知道**完全条件概率(full conditionals)**，这里即：$p(z_i=k|\\vec z_{-i}, \\vec w)$。设观测到的词$w_i=t$，根据贝叶斯法则，有：$p(z_i=k|\\vec z_{-i}, \\vec w)\\propto p(z_i=k, w_i=t|\\vec z_{-i}, \\vec w_{-i})$\n",
    "\n",
    "**完整推导**：\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/lda_gibbs.png)\n",
    "\n",
    "### 最终的Gibbs Smapling公式\n",
    "\n",
    "$$p(z_i=k|\\vec z_{-i}, \\vec w)\\propto \\frac{n^{k}_{m,-i}+\\alpha_k}{\\sum_{k=1}^K(n^{k}_{m,-i}+\\alpha_k)} \\cdot \\frac{n^{t}_{k,-i}+\\beta_t}{\\sum_{t=1}^V(n^{t}_{k,-i}+\\beta_t)}$$\n",
    "\n",
    "右边是$p(topic|doc)\\cdot p(word|topic)$，这个概率其实是$doc\\rightarrow topic \\rightarrow word$的路径概率。\n",
    "\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/doc-topic-word.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA训练\n",
    "\n",
    "LDA训练算法：\n",
    "- 1.随机初始化：对语料中每篇文档的每个词$w$，随机赋一个topic编号$z$。\n",
    "- 2.重新扫描语料库，对每个词$w$，按照Gibbs Sampling公式重新采样它的topic，在语料中进行更新。\n",
    "- 3.重复上面的过程直到Gibbs Sampling收敛。\n",
    "- 4.统计语料库的topic-word共现频率矩阵，该矩阵就是LDA模型。\n",
    "\n",
    "## LDA的inference\n",
    "\n",
    "LDA的inference：\n",
    "- 1.随机初始化：当前文档的每个词$w$，随机赋一个topic编号$z$。\n",
    "- 2.重新当前文档，对每个词$w$，按照Gibbs Sampling公式重新采样它的topic。\n",
    "- 3.重复上面的过程直到Gibbs Sampling收敛。\n",
    "- 4.统计当前文档中的topic分布，该分部就是$\\vec \\theta_{new}$。\n",
    "\n",
    "## LDA实现\n",
    "\n",
    "投骰子程序（累加法），参考[之前的博客](https://applenob.github.io/1_MCMC.html#离散分布采样)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_discrete(vec):\n",
    "    if sum(vec) != 1:\n",
    "        vec = vec / sum(vec)\n",
    "    u = np.random.rand()\n",
    "    start = 0\n",
    "    for i, num in enumerate(vec):      \n",
    "        if u > start:\n",
    "            start += num\n",
    "        else:\n",
    "            return i-1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将最终的Gibbs Sampling的公式换成代码里的变量：\n",
    "\n",
    "$$p(z_i=k|\\vec z_{-i}, \\vec w)\\propto \\frac{nd[m][k]+\\alpha}{ndsum[m]+K\\alpha} \\cdot \\frac{nw[wordid][k]+\\beta}{nwsum[k]+V\\beta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_train(doc_set, word2id, K, alpha=1.0, beta=1.0, iter_number=200, with_debug_log=True, check_every=10):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    doc_set: 分词后的语料库。\n",
    "    word2id: 单词到单词id的映射。\n",
    "    K: 主题数。\n",
    "    alpha: doc-topic先验参数。\n",
    "    beta: topic-word先验参数。\n",
    "    iter_num: 迭代次数。\n",
    "    \n",
    "    output：\n",
    "    theta: size M×K（doc->topic）。\n",
    "    phi: size K×V（topic->word）。\n",
    "    tassign文件（topic assignment）。\n",
    "    \n",
    "    重要变量：\n",
    "    nw：size：V×K，表示第i个词assign到第j个topic的个数。\n",
    "    nwsum：size：K，表示assign到第j个topic的所有词的个数。\n",
    "    nd：size：M×K，表示第i个文档中，第j个topic的词出现的个数。\n",
    "    ndsum：size：M，表示第i个文档中所有词的的个数。\n",
    "    z：size：M×per_doc_word_len，表示第m篇文档的第n个word被指定的topic id。\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"init ...\")\n",
    "    M = np.shape(doc_set)[0]\n",
    "    N = max(map(len, doc_set))\n",
    "    V = len(word2id)\n",
    "    nw = np.zeros((V, K), dtype=int)\n",
    "    nwsum = np.zeros(K, dtype=int)\n",
    "    nd = np.zeros((M, K), dtype=int)\n",
    "    ndsum = np.zeros(M, dtype=int)\n",
    "    z = np.zeros((M, N), dtype=int)\n",
    "    theta = np.zeros((M, K))\n",
    "    phi = np.zeros((K, V))\n",
    "    \n",
    "    # 初始化阶段\n",
    "    for m, doc in enumerate(doc_set):\n",
    "        for n, word in enumerate(doc):\n",
    "            topic_id = np.random.randint(0, K)  # 初始化阶段随机指定\n",
    "            word_id = word2id[word]\n",
    "            z[m][n] = topic_id  # 将随机产生的主题存入z中\n",
    "            nw[word_id][topic_id] += 1  # 为相应的统计量+1\n",
    "            nwsum[topic_id] += 1\n",
    "            nd[m][topic_id] += 1\n",
    "            ndsum[m] += 1\n",
    "    if with_debug_log:\n",
    "        id2word = dict([(v, k) for k, v in word2id.items()])\n",
    "        print(\"nw: \", nw, \"nd: \", nd, \"nwsum: \", nwsum, \"ndsum: \", ndsum)\n",
    "            \n",
    "    print(\"start iterating ...\")\n",
    "    # Gibbs Sampling迭代阶段\n",
    "    for one_iter in range(iter_number):\n",
    "        ss = time.time()\n",
    "        for m, doc in enumerate(doc_set):\n",
    "            for n, word in enumerate(doc):\n",
    "                word_id = word2id[word]\n",
    "                t = z[m][n]\n",
    "                nw[word_id][t] -= 1\n",
    "                nwsum[t] -= 1\n",
    "                nd[m][t] -= 1\n",
    "                p = [0 for _ in range(K)]  # 存放各主题概率\n",
    "                for k in range(K):\n",
    "                    p[k] = float(nd[m][k] + alpha) / (ndsum[m] + K * alpha) * \\\n",
    "                           float(nw[word_id][k] + beta) / (nwsum[k] + V * beta)\n",
    "                # 重新采样\n",
    "                new_t = sample_discrete(p)\n",
    "                z[m][n] = new_t\n",
    "                nw[word_id][new_t] += 1\n",
    "                nwsum[new_t] += 1\n",
    "                nd[m][new_t] += 1\n",
    "        if with_debug_log and (one_iter + 1) % check_every == 0:\n",
    "            print(\"iter #\", one_iter, \" time cost: \", time.time() - ss)\n",
    "            print(\"nw: \", nw, \"nd: \", nd, \"nwsum: \", nwsum, \"ndsum: \", ndsum)\n",
    "            for m in range(M):\n",
    "                for k in range(K):\n",
    "                    theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha)\n",
    "            for k in range(K):\n",
    "                for v in range(V):\n",
    "                    phi[k][v] = (nw[v][k] + beta) / (nwsum[k] + V * beta)\n",
    "            for one_topic in phi:\n",
    "                print(one_topic.argsort()[: -21: -1])\n",
    "                print(\" \".join([id2word[i] for i in one_topic.argsort()[: -21: -1]]))\n",
    "                print(\"calculating theta and phi\")\n",
    "    # 最后计算最终的theta和phi矩阵\n",
    "    for m in range(M):\n",
    "        for k in range(K):\n",
    "            theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha)\n",
    "    for k in range(K):\n",
    "        for v in range(V):\n",
    "            phi[k][v] = (nw[v][k] + beta) / (nwsum[k] + V * beta)\n",
    "#     print(\"z(tassign.txt): \", z)\n",
    "    return z, theta, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "由于python版的程序运行较慢，为了演示效果，使用较少的数据，这里用100篇doc。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.732 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "      <th>cut_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>资料图：据台湾检方初步掌握资料显示，死者大多陈尸在游览车后座区，分析想从后座逃离，但无法破窗...</td>\n",
       "      <td>新闻</td>\n",
       "      <td>资料 图 ： 据 台湾 检方 初步 掌握 资料 显示 ， 死者 大多 陈尸 在 游览车 后座...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>导语：“性冷淡风”风靡时髦客的时间很久依然受宠到极致，究其原因，除了简洁的外观外，还有一种在...</td>\n",
       "      <td>时尚</td>\n",
       "      <td>导语 ： “ 性冷淡 风 ” 风靡 时髦 客 的 时间 很 久 依然 受宠 到 极致 ， 究...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>凤凰体育讯 北京时间7月19日晚，2016“意大利传奇巨星中国行”中意明星元老对抗赛荔波战在...</td>\n",
       "      <td>体育</td>\n",
       "      <td>凤凰 体育讯   北京 时间 7 月 19 日晚 ， 2016 “ 意大利 传奇 巨星 中国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>作为盗墓笔记的前传，《老九门》昨晚终于播了。22点守候在电机前的哔宝看了首播只想说，好吓人，...</td>\n",
       "      <td>娱乐</td>\n",
       "      <td>作为 盗墓 笔记 的 前 传 ， 《 老 九门 》 昨晚 终于 播 了 。 22 点 守候 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>火箭熊凤凰体育讯 北京时间7月23日，根据ClutchFans报道，一位火箭内部的消息人士确...</td>\n",
       "      <td>体育</td>\n",
       "      <td>火箭 熊 凤凰 体育讯   北京 时间 7 月 23 日 ， 根据 ClutchFans 报...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content class  \\\n",
       "4117  资料图：据台湾检方初步掌握资料显示，死者大多陈尸在游览车后座区，分析想从后座逃离，但无法破窗...    新闻   \n",
       "1244  导语：“性冷淡风”风靡时髦客的时间很久依然受宠到极致，究其原因，除了简洁的外观外，还有一种在...    时尚   \n",
       "3131  凤凰体育讯 北京时间7月19日晚，2016“意大利传奇巨星中国行”中意明星元老对抗赛荔波战在...    体育   \n",
       "1602  作为盗墓笔记的前传，《老九门》昨晚终于播了。22点守候在电机前的哔宝看了首播只想说，好吓人，...    娱乐   \n",
       "3964  火箭熊凤凰体育讯 北京时间7月23日，根据ClutchFans报道，一位火箭内部的消息人士确...    体育   \n",
       "\n",
       "                                               cut_text  \n",
       "4117  资料 图 ： 据 台湾 检方 初步 掌握 资料 显示 ， 死者 大多 陈尸 在 游览车 后座...  \n",
       "1244  导语 ： “ 性冷淡 风 ” 风靡 时髦 客 的 时间 很 久 依然 受宠 到 极致 ， 究...  \n",
       "3131  凤凰 体育讯   北京 时间 7 月 19 日晚 ， 2016 “ 意大利 传奇 巨星 中国...  \n",
       "1602  作为 盗墓 笔记 的 前 传 ， 《 老 九门 》 昨晚 终于 播 了 。 22 点 守候 ...  \n",
       "3964  火箭 熊 凤凰 体育讯   北京 时间 7 月 23 日 ， 根据 ClutchFans 报...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "copus_name = \"~/Data/nlp/fenghuang.csv\"\n",
    "copus_df = pd.read_csv(copus_name).sample(frac=0.02, replace=True)\n",
    "\n",
    "def cut_text_with_jieba(text):\n",
    "    return \" \".join(jieba.cut(text, cut_all=False))\n",
    "\n",
    "copus_df[\"cut_text\"] = copus_df[\"content\"].apply(cut_text_with_jieba)\n",
    "copus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8197 [('资料', 0), ('图', 1), ('：', 2), ('据', 3), ('台湾', 4), ('检方', 5), ('初步', 6), ('掌握', 7), ('显示', 8), ('，', 9)]\n"
     ]
    }
   ],
   "source": [
    "word2id = {}\n",
    "for doc in copus_df[\"cut_text\"].values:\n",
    "    for word in doc.split():\n",
    "        if word not in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "print(len(word2id), list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['资料', '图', '：', '据', '台湾', '检方', '初步', '掌握', '资料', '显示']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copus_list = copus_df[\"cut_text\"].apply(lambda doc: doc.split()).values\n",
    "copus_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init ...\n",
      "start iterating ...\n",
      "time cost:  973.8627579212189\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "z, theta, phi = lda_train(copus_list, word2id, 10, with_debug_log=False)\n",
    "# print(\"z: \", z, \"theta: \", theta, \"phi: \", phi)\n",
    "print(\"time cost: \", time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95, 10), (10, 8197))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape, phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict([(v, k) for k, v in word2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00308642  0.01234568  0.00925926  0.00308642  0.00925926  0.01851852\n",
      "  0.00308642  0.58641975  0.0308642   0.32407407] 7\n"
     ]
    }
   ],
   "source": [
    "print(theta[0], np.argmax(theta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 267   82 7327 5828 3159 5827 2405  475  242 3198  290  259  311  233 1719\n",
      "  237 3167  253 7328 3162]\n",
      "- 、 伊斯 人队 布朗 76 号 赛季 元老 NBA 6 队 11 19 分 传奇 合同 巴乔 科 火箭队\n",
      "[2515 2516 2524 2521 2519 2517 2610 2583 2514 2496 2523 2636 2654   99 2844\n",
      " 2532 1937  675 2892 2996]\n",
      "高宗 孝宗 退位 内禅 皇帝 光宗 即位 太子 徽宗 父亲 生前 金国 宋高宗 与 去世 两宋 不确定性 命运 极端 严重\n",
      "[2323 1647 4289 4300 4341 4299 4304  477   45 5453 2536 5362 7016 3617 5449\n",
      " 1846 4303 6963   96 4308]\n",
      "系统 部署 萨德 反导 朝鲜 美韩 半岛 可能 月 类人 官方 盘初 裸色 打击 另一半 回答 利益 伤害 一份 坚决\n",
      "[1869  176 7448 1909 2313 1917 1912 7453 1930 1908 1929 7452 2215 2214 1949\n",
      " 1860  149 1820 1816 1822]\n",
      ", 公司 万科 地产 ; 保利 停牌 证监局 业务 中航 开发 深圳 设备 生产 整合 : 相关 唐纳 地图 记者会\n",
      "[3610 2354 3588 3587 3571 4390 3605 3931 1860 3639 1531 3901  897 5191 3937\n",
      " 6465 1220 4498 3612 3565]\n",
      "美元 / ) ( 原油 英镑 下跌 库存 : 汽油 至 贸易 报 指数 高位 新西兰 或 预期 反弹 油价\n",
      "[1446 7888 7050 4890 1180 7889 7051 1197 5002 1756 1224  361  505 7047 1276\n",
      " 4986 1345 1284  364 3388]\n",
      "长征 哈达铺 马思纯 将军 战狼 纪念馆 盛一伦 吴京 团 成员 演员 《 角色 侣 导演 参观 视频 动作 》 图为\n",
      "[4108 4105 5226 4106 4109 1913 6855 2346 4146 2362 4206  372  187 2332 2057\n",
      " 4121 4120 4119 4275 4129]\n",
      "全聚德 高管 关晓彤 集体 股价 公告 ILO 争议 请辞 高 式 哔宝 时髦 探索 上赛季 邢颖 总经理 王志强 腾讯 职务\n",
      "[  9 103  27  13 368  48  50  82 166 203  45   2 139 472 172 177  44  95\n",
      "  99 127]\n",
      "， 的 。 在 了 “ ” 、 是 和 月 ： 我 他 也 将 7 上 与 有\n",
      "[6743 8060 8061 1835 8062 6768 6750 4682 8101 6737 8092 6739 4685 3181 4690\n",
      " 4700 8065 3800 6744 2370]\n",
      "棋手 吉祥 宝宝 太平岛 食神 组 本赛 江启臣 厨神 杯 腊肠 预赛 冯世宽 东莞 防务 考察 三维动画 大战 普通 32\n",
      "[5746 5745  120   65 2215 5226 7500 7501   83   34 3619 8142 7519   70 7363\n",
      " 4562 8145 4070   33  813]\n",
      "充电 无线 技术 录音 设备 关晓彤 罹难者 家属 矮仔 事故 去年 电动汽车 理赔 肇事 肖天 装置 用电 能量 台 提升\n"
     ]
    }
   ],
   "source": [
    "for one_topic in phi:\n",
    "    print(one_topic.argsort()[: -21: -1])\n",
    "    print(\" \".join([id2word[i] for i in one_topic.argsort()[: -21: -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "从这个Demo看，还是make sense的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
