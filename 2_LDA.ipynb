{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA主题模型学习总结\n",
    "\n",
    "`本篇博客是《LDA漫游指南》和《LDA数学八卦》的学习笔记。`\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [简介](#简介)\n",
    "    - [LDA算法输入与输出](#LDA算法输入与输出)\n",
    "- [前置知识](#前置知识)\n",
    "    - [gamma函数](#gamma函数)\n",
    "    - [二项分布](#二项分布)\n",
    "    - [Beta分布](#Beta分布)\n",
    "    - [多项分布](#多项分布)\n",
    "    - [Dirichlet分布](#Dirichlet分布)\n",
    "    - [共轭先验分布](#共轭先验分布)\n",
    "    - [MCMC](#MCMC)\n",
    "- [LDA推导](#LDA推导)\n",
    "    - [贝叶斯unigram](#贝叶斯unigram)\n",
    "    - [LDA模型的标准生成过程](#LDA模型的标准生成过程)\n",
    "    - [数学表示](#数学表示)\n",
    "- [交给Gibbs Sampling](#交给Gibbs-Sampling)\n",
    "    - [最终的Gibbs Smapling公式](#最终的Gibbs-Smapling公式)\n",
    "- [LDA训练](#LDA训练)\n",
    "- [LDA的inference](#LDA的inference)\n",
    "- [LDA实现](#LDA实现)\n",
    "    \n",
    "\n",
    "## 简介\n",
    "\n",
    "LDA（Latent Dirichlet Allocation）是一种**非监督**机器学习技术，可以用来识别大规模文档集或语料库中潜在隐藏的主题信息。\n",
    "\n",
    "LDA假设每个词是由背后的一个潜在隐藏的主题中抽取出来的，对于每篇文档，生成过程如下：\n",
    "- 1.对于每篇文档，从主题分布中抽取一个主题。\n",
    "- 2.从上述被抽到的主题所对应的单词分布中抽取一个单词。\n",
    "- 3.重复上述过程直到遍历文档中的每个单词。\n",
    "\n",
    "### LDA算法输入与输出\n",
    "- 输入：分词后的文章集。主题数$K$，超参数：$\\alpha$和$\\beta$。\n",
    "- 输出：\n",
    "    - 1.每篇文章每个词被指定的主题编号。\n",
    "    - 2.每篇文章的主题概率分布：$\\theta$\n",
    "    - 3.每个主题下的词概率分布：$\\phi$\n",
    "    - 4.词和id的映射表。\n",
    "    - 5.每个主题$\\phi$下\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置知识\n",
    "\n",
    "### gamma函数\n",
    "\n",
    "所谓的gamma函数就是阶乘的函数形式。\n",
    "\n",
    "$$\\Gamma(x)=\\int_0^{+\\infty}e^{-t}t^{x-1}dt\\;\\;\\;(x>0)$$\n",
    "\n",
    "$$\\Gamma(n) = (n-1)!$$\n",
    "\n",
    "### 二项分布\n",
    "\n",
    "打靶，$n$次中中了$k$次的概率：\n",
    "\n",
    "$$f(k;n,p)=Pr(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}$$\n",
    "\n",
    "### Beta分布\n",
    "\n",
    "$X\\sim Beta(\\alpha, \\beta)$\n",
    "\n",
    "概率密度函数：$$f(x;\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\\\=\\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
    "\n",
    "期望：$$E(p) = \\int_0^1t\\cdot Beta(t|\\alpha, \\beta)dt\\\\=\\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "\n",
    "### 多项分布\n",
    "\n",
    "多项分布是二项分布的推广：投$n$次骰子，共有六种结果，概率为$p_i$，$i$点出现$x_i$次的组合概率：\n",
    "\n",
    "$$f(x_1, ...x_k;n,p_1,...,p_k)=Pr(X_1=x_1\\; and\\; ... and\\; X_k=x_k)\\\\=\\frac{n!}{x_1!...x_k!}p_1^{x_1}...p_k^{x_k}\\;\\;\\;when\\;\\sum_{i=1}^kx_i = n$$\n",
    "\n",
    "### Dirichlet分布\n",
    "\n",
    "$$p\\sim D(t|\\alpha)$$\n",
    "\n",
    "概率密度函数：$$f(p_1,..., p_k-1)=\\frac{1}{\\Delta (\\alpha)}\\prod_{i=1}^kp_i^{\\alpha_i-1}$$\n",
    "\n",
    "期望：$$E(p) = (\\frac{\\alpha_1}{\\sum_{i=1}^K\\alpha_i}, \\frac{\\alpha_2}{\\sum_{i=1}^K\\alpha_i}, ..., \\frac{\\alpha_K}{\\sum_{i=1}^K\\alpha_i})$$\n",
    "\n",
    "### 共轭先验分布\n",
    "\n",
    "贝叶斯公式：$$p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}$$\n",
    "\n",
    "即：**后验分布=似然函数×先验分布**\n",
    "\n",
    "**共轭**：选取一个函数作为似然函数，使得先验分布函数和后验分布函数的形式一致。\n",
    "\n",
    "- beta分布是二项分布的共轭先验分布，即，二项分布作为似然函数，先验分布是beta分布，后验分布依然是beta分布。\n",
    "- Dirichlet分布是多项式分布的共轭先验分布，即，多项式布作为似然函数，先验分布是Dirichlet分布，后验分布依然是Dirichlet分布。\n",
    "\n",
    "### MCMC\n",
    "\n",
    "参考之前的博客：https://applenob.github.io/1_MCMC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA推导\n",
    "\n",
    "### 贝叶斯unigram\n",
    "\n",
    "不考虑单词简单顺序，被称为“词袋模型”。\n",
    "\n",
    "$$P(W) = p(w_1)p(w_2)...p(w_n) = \\prod^V_{t=1}p_t^{n_t}\\;\\;\\;\\sum^V_{t=1}p_t = 1$$\n",
    "\n",
    "为什么似然是多项式分布？想象一个巨大的骰子，有$V$个面，每面代表一个词，每个面的概率是$\\vec{p}=(p_1, ...p_V)$，产生次数是：$\\vec{n} = (n_1, ..., n_V)$，那么生成某篇文章的概率是服从多项式分布的。\n",
    "\n",
    "贝叶斯学派认为参数也服从某种分布，即，不知道上帝用哪个骰子来生成文档，这个选取骰子的概率，服从Dirichlet分布。\n",
    "\n",
    "又有：$Dir(\\vec{p}|\\vec{\\alpha}) + MultCount(\\vec{n}) = Dir(\\vec{p}|\\vec{\\alpha}+\\vec{n})$，综合上面Dirichlet分布的期望，可以得到对于每一个$p_i$，可以如下**估计**：$\\tilde p_i = \\frac{n_i+\\alpha_i}{\\sum_{i=1}^V(n_i + \\alpha_i)}$。即，每个参数的估计值是其对应事件的先验的伪计数和数据中的计数的和在整体技术中的比例。\n",
    "\n",
    "进一步，计算出**文本语料的产生概率**是：$p(W|\\vec{\\alpha}) = \\int p(W|\\vec{p})p(\\vec{p}|\\vec{\\alpha})d\\vec{p}=\\frac{\\Delta(\\vec{n}+\\vec{\\alpha})}{\\Delta \\vec{\\alpha}}$\n",
    "\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/bayes_unigram.png)\n",
    "\n",
    "用通俗的话说，这是上帝从一个坛子中抽一个骰子，再丢这个骰子，观察结果的过程。\n",
    "\n",
    "### LDA模型的标准生成过程\n",
    "\n",
    "LDA相当于两个上面的步骤的结合（两个坛子）。上帝有两个大坛子，第一个坛子装doc-topic骰子，第二个坛子装topic-word骰子：\n",
    "\n",
    "- 1.选择$\\theta_i \\sim Dir(\\vec{\\alpha})$，这里$i\\in\\{1,2,...,M\\}$，$M$代表文章数。每生成一篇文章，从第一个坛子中选一个doc-topic骰子。\n",
    "- 2.选择$\\phi_i \\sim Dir(\\vec{\\beta})$，这里$k \\in \\{1,2,...,K\\}$，$K$代表主题个数。独立地挑了$K$个topic-word骰子。\n",
    "- 3.对每个单词的位置$W_{i,j}$，这里$j \\in \\{1,...,N_i\\}$，$i \\in \\{1,...,M\\}$\n",
    "    - 4.选择一个topic主题：$z_{i,j} \\sim Multinominal(\\theta_i)$。投掷这个doc-topic骰子，得到一个topic编号$z$。\n",
    "    - 5.选择一个word词：$w_{i,j} \\sim Multinominal(\\phi_{z_{i,j}})$。投掷topic是$z$的topic-word骰子，得到一个词。\n",
    "    \n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/lda.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 数学表示\n",
    "\n",
    "对每个doc-topic骰子，有：$p(\\vec z_m | \\vec \\alpha) = \\frac{\\Delta(\\vec n_m +\\vec  \\alpha)}{\\Delta \\vec \\alpha}$\n",
    "\n",
    "其中：$\\vec n_m = (n_m^{(1)}, .., n_m^{(K)})$，$n_m^{(k)}$代表第$m$篇文档中第$k$个topic产生的词的个数。$\\vec \\alpha$是$K$维的向量。\n",
    "\n",
    "因为M篇文章生成topic的过程上**相互独立**的，有M个doc-topic骰子的联合概率分布，即**整个语料的topics生成概率**：$$p(\\vec z| \\vec \\alpha) = \\prod^M_{m=1}p(\\vec z_m|\\vec \\alpha)\\\\=\\prod^M_{m=1}\\frac{\\Delta(\\vec n_m + \\vec \\alpha)}{\\Delta \\vec \\alpha}$$\n",
    "\n",
    "对每topic-word骰子，有：$p(\\vec w_k|\\vec \\beta) = \\frac{\\Delta(\\vec n_k + \\vec \\beta)}{\\Delta \\vec \\beta}$\n",
    "\n",
    "其中：$\\vec n_k = (n_k^{(1)}, .., n_k^{(V)})$，$n_k^{(v)}$代表第$k$个topic产生的词中，第$v$个word产生的词的个数。$\\vec \\beta\n",
    "$是$V$维的向量。\n",
    "\n",
    "因为K个topic生成word的过程也是**相互独立**的，有K个topic-word骰子的联合概率分布，即**整个语料中words生成的概率**：$$p(\\vec w|\\vec z, \\vec \\beta)\\\\=\\prod_{k=1}^Kp(\\vec w_{(k)}|\\vec z_{(k)}, \\vec \\beta)\\\\=\\prod_{k=1}^K\\frac{\\Delta(\\vec n_k + \\vec \\beta)}{\\Delta \\vec \\beta}$$\n",
    "\n",
    "联合上面两个联合概率分布，得到**整个语料中words生成的概率**和**整个语料的topics生成概率**的**联合概率分布**：\n",
    "\n",
    "$$p(\\vec w, \\vec z| \\vec \\alpha, \\vec \\beta)\\\\=p(\\vec w|\\vec z, \\vec \\beta)p(\\vec z| \\vec \\alpha)\\\\=\\prod_{k=1}^K\\frac{\\Delta(\\vec n_k + \\vec \\beta)}{\\Delta \\vec \\beta}\\prod^M_{m=1}\\frac{\\Delta(\\vec n_m + \\vec \\alpha)}{\\Delta \\vec \\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 交给Gibbs Sampling\n",
    "\n",
    "Gibbs Smapling建议先回顾下之前的[博客文章](https://applenob.github.io/1_MCMC.html#Gibbs-Sampling)。\n",
    "\n",
    "有了联合分布$p(\\vec w, \\vec z)$，可以使用Gibbs Sampling了。\n",
    "\n",
    "我们的**终极目标**是：要使用一个马尔科夫链，sample出一些列的状态点，使得最终的平稳分布状态就是我们给定的联合概率分布。\n",
    "\n",
    "语料库中第$i$个词对应的topic记为$z_i$，其中$i=(m,n)$是一个二维下标，对应第$m$篇文档的第$n$个词。$-i$表示去除下标$i$的词。\n",
    "\n",
    "我们要采样的分布是$p(\\vec z| \\vec w)$，根据Gibbs Sampling的要求，我们要知道**完全条件概率(full conditionals)**，这里即：$p(z_i=k|\\vec z_{-i}, \\vec w)$。设观测到的词$w_i=t$，根据贝叶斯法则，有：$p(z_i=k|\\vec z_{-i}, \\vec w)\\propto p(z_i=k, w_i=t|\\vec z_{-i}, \\vec w_{-i})$\n",
    "\n",
    "**完整推导**：\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/lda_gibbs.png)\n",
    "\n",
    "### 最终的Gibbs Smapling公式\n",
    "\n",
    "$$p(z_i=k|\\vec z_{-i}, \\vec w)\\propto \\frac{n^{k}_{m,-i}+\\alpha_k}{\\sum_{k=1}^K(n^{k}_{m,-i}+\\alpha_k)} \\cdot \\frac{n^{t}_{k,-i}+\\beta_t}{\\sum_{t=1}^V(n^{t}_{k,-i}+\\beta_t)}$$\n",
    "\n",
    "右边是$p(topic|doc)\\cdot p(word|topic)$，这个概率其实是$doc\\rightarrow topic \\rightarrow word$的路径概率。\n",
    "\n",
    "![](https://github.com/applenob/machine_learning_basic/raw/master/res/doc-topic-word.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA训练\n",
    "\n",
    "LDA训练算法：\n",
    "- 1.随机初始化：对语料中每篇文档的每个词$w$，随机赋一个topic编号$z$。\n",
    "- 2.重新扫描语料库，对每个词$w$，按照Gibbs Sampling公式重新采样它的topic，在语料中进行更新。\n",
    "- 3.重复上面的过程直到Gibbs Sampling收敛。\n",
    "- 4.统计语料库的topic-word共现频率矩阵，该矩阵就是LDA模型。\n",
    "\n",
    "## LDA的inference\n",
    "\n",
    "LDA的inference：\n",
    "- 1.随机初始化：当前文档的每个词$w$，随机赋一个topic编号$z$。\n",
    "- 2.重新当前文档，对每个词$w$，按照Gibbs Sampling公式重新采样它的topic。\n",
    "- 3.重复上面的过程直到Gibbs Sampling收敛。\n",
    "- 4.统计当前文档中的topic分布，该分部就是$\\vec \\theta_{new}$。\n",
    "\n",
    "## LDA实现\n",
    "\n",
    "投骰子程序（累加法），参考[之前的博客](https://applenob.github.io/1_MCMC.html#离散分布采样)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_discrete(vec):\n",
    "    if sum(vec) != 1:\n",
    "        vec = vec / sum(vec)\n",
    "    u = np.random.rand()\n",
    "    start = 0\n",
    "    for i, num in enumerate(vec):      \n",
    "        if u > start:\n",
    "            start += num\n",
    "        else:\n",
    "            return i-1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将最终的Gibbs Sampling的公式换成代码里的变量：\n",
    "\n",
    "$$p(z_i=k|\\vec z_{-i}, \\vec w)\\propto \\frac{nd[m][k]+\\alpha}{ndsum[m]+K\\alpha} \\cdot \\frac{nw[wordid][k]+\\beta}{nwsum[k]+V\\beta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_train(doc_set, word2id, K, alpha=1.0, beta=1.0, iter_number=10, with_debug_log=True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    doc_set: 分词后的语料库。\n",
    "    word2id: 单词到单词id的映射。\n",
    "    K: 主题数。\n",
    "    alpha: doc-topic先验参数。\n",
    "    beta: topic-word先验参数。\n",
    "    iter_num: 迭代次数。\n",
    "    \n",
    "    output：\n",
    "    theta: size M×K（doc->topic）。\n",
    "    phi: size K×V（topic->word）。\n",
    "    tassign文件（topic assignment）。\n",
    "    \n",
    "    重要变量：\n",
    "    nw：size：V×K，表示第i个词assign到第j个topic的个数。\n",
    "    nwsum：size：K，表示assign到第j个topic的所有词的个数。\n",
    "    nd：size：M×K，表示第i个文档中，第j个topic的词出现的个数。\n",
    "    ndsum：size：M，表示第i个文档中所有词的的个数。\n",
    "    z：size：M×per_doc_word_len，表示第m篇文档的第n个word被指定的topic id。\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"init ...\")\n",
    "    M = np.shape(doc_set)[0]\n",
    "    N = max(map(len, doc_set))\n",
    "    V = len(word2id)\n",
    "    nw = np.zeros((V, K), dtype=int)\n",
    "    nwsum = np.zeros(K, dtype=int)\n",
    "    nd = np.zeros((M, K), dtype=int)\n",
    "    ndsum = np.zeros(M, dtype=int)\n",
    "    z = np.zeros((M, N), dtype=int)\n",
    "    theta = np.zeros((M, K))\n",
    "    phi = np.zeros((K, V))\n",
    "    \n",
    "    # 初始化阶段\n",
    "    for m, doc in enumerate(doc_set):\n",
    "        for n, word in enumerate(doc):\n",
    "            topic_id = np.random.randint(0, K)  # 初始化阶段随机指定\n",
    "            word_id = word2id[word]\n",
    "            z[m][n] = topic_id  # 将随机产生的主题存入z中\n",
    "            nw[word_id][topic_id] += 1  # 为相应的统计量+1\n",
    "            nwsum[topic_id] += 1\n",
    "            nd[m][topic_id] += 1\n",
    "            ndsum[m] += 1\n",
    "    if with_debug_log:\n",
    "        id2word = dict([(v, k) for k, v in word2id.items()])\n",
    "        print(\"nw: \", nw, \"nd: \", nd, \"nwsum: \", nwsum, \"ndsum: \", ndsum)\n",
    "            \n",
    "    print(\"start iterating ...\")\n",
    "    # Gibbs Sampling迭代阶段\n",
    "    for one_iter in range(iter_number):\n",
    "        print(\"iterating #\", one_iter, \" ...\")\n",
    "        ss = time.time()\n",
    "        for m, doc in enumerate(doc_set):\n",
    "            for n, word in enumerate(doc):\n",
    "                word_id = word2id[word]\n",
    "                t = z[m][n]\n",
    "                nw[word_id][t] -= 1\n",
    "                nwsum[t] -= 1\n",
    "                nd[m][t] -= 1\n",
    "                p = [0 for _ in range(K)]  # 存放各主题概率\n",
    "                for k in range(K):\n",
    "                    p[k] = float(nd[m][k] + alpha) / (ndsum[m] + K * alpha) * \\\n",
    "                           float(nw[word_id][k] + beta) / (nwsum[k] + V * beta)\n",
    "                # 重新采样\n",
    "                new_t = sample_discrete(p)\n",
    "                z[m][n] = new_t\n",
    "                nw[word_id][new_t] += 1\n",
    "                nwsum[new_t] += 1\n",
    "                nd[m][new_t] += 1\n",
    "        if with_debug_log:\n",
    "            print(\"iter time cost: \", time.time() - ss)\n",
    "            print(\"nw: \", nw, \"nd: \", nd, \"nwsum: \", nwsum, \"ndsum: \", ndsum)\n",
    "            for m in range(M):\n",
    "                for k in range(K):\n",
    "                    theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha)\n",
    "            for k in range(K):\n",
    "                for v in range(V):\n",
    "                    phi[k][v] = (nw[v][k] + beta) / (nwsum[k] + V * beta)\n",
    "            for one_topic in phi:\n",
    "                print(one_topic.argsort()[: -21: -1])\n",
    "                print(\" \".join([id2word[i] for i in one_topic.argsort()[: -21: -1]]))\n",
    "                print(\"calculating theta and phi\")\n",
    "    # 最后计算最终的theta和phi矩阵\n",
    "    for m in range(M):\n",
    "        for k in range(K):\n",
    "            theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha)\n",
    "    for k in range(K):\n",
    "        for v in range(V):\n",
    "            phi[k][v] = (nw[v][k] + beta) / (nwsum[k] + V * beta)\n",
    "#     print(\"z(tassign.txt): \", z)\n",
    "    return z, theta, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.646 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "      <th>cut_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>货币:欧元/美元 阻力位2：1.1240 阻力位1：1.1120 即期价格:1.1070 支...</td>\n",
       "      <td>财经</td>\n",
       "      <td>货币 : 欧元 / 美元   阻力位 2 ： 1.1240   阻力位 1 ： 1.1120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FX168财经报社(香港)讯 德国ZEW经济景气指数面向金融专家，调查他们基于通胀、汇率和股...</td>\n",
       "      <td>财经</td>\n",
       "      <td>FX168 财经 报社 ( 香港 ) 讯   德国 ZEW 经济 景气 指数 面向 金融 专...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>据国资委网站消息，经营性国有资产集中统一监管稳步推进。目前，全国经营性国有资产集中统一监管占...</td>\n",
       "      <td>财经</td>\n",
       "      <td>据 国资委 网站 消息 ， 经营性 国有资产 集中统一 监管 稳步 推进 。 目前 ， 全国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FX168财经报社(香港)讯 7月15日，由北京朝阳海外学人创业大会主办，学无国界旗下UVI...</td>\n",
       "      <td>财经</td>\n",
       "      <td>FX168 财经 报社 ( 香港 ) 讯   7 月 15 日 ， 由 北京 朝阳 海外学人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者：李振 来源：发展观察家（guanchajia010）12年有多久，笔者从青葱少年变成了...</td>\n",
       "      <td>财经</td>\n",
       "      <td>作者 ： 李振   来源 ： 发展 观察家 （ guanchajia010 ） 12 年 有...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content class  \\\n",
       "0  货币:欧元/美元 阻力位2：1.1240 阻力位1：1.1120 即期价格:1.1070 支...    财经   \n",
       "1  FX168财经报社(香港)讯 德国ZEW经济景气指数面向金融专家，调查他们基于通胀、汇率和股...    财经   \n",
       "2  据国资委网站消息，经营性国有资产集中统一监管稳步推进。目前，全国经营性国有资产集中统一监管占...    财经   \n",
       "3  FX168财经报社(香港)讯 7月15日，由北京朝阳海外学人创业大会主办，学无国界旗下UVI...    财经   \n",
       "4  作者：李振 来源：发展观察家（guanchajia010）12年有多久，笔者从青葱少年变成了...    财经   \n",
       "\n",
       "                                            cut_text  \n",
       "0  货币 : 欧元 / 美元   阻力位 2 ： 1.1240   阻力位 1 ： 1.1120...  \n",
       "1  FX168 财经 报社 ( 香港 ) 讯   德国 ZEW 经济 景气 指数 面向 金融 专...  \n",
       "2  据 国资委 网站 消息 ， 经营性 国有资产 集中统一 监管 稳步 推进 。 目前 ， 全国...  \n",
       "3  FX168 财经 报社 ( 香港 ) 讯   7 月 15 日 ， 由 北京 朝阳 海外学人...  \n",
       "4  作者 ： 李振   来源 ： 发展 观察家 （ guanchajia010 ） 12 年 有...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "copus_name = \"~/Data/nlp/fenghuang.csv\"\n",
    "copus_df = pd.read_csv(copus_name)\n",
    "\n",
    "def cut_text_with_jieba(text):\n",
    "    return \" \".join(jieba.cut(text, cut_all=False))\n",
    "\n",
    "copus_df[\"cut_text\"] = copus_df[\"content\"].apply(cut_text_with_jieba)\n",
    "copus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106280 [('货币', 0), (':', 1), ('欧元', 2), ('/', 3), ('美元', 4), ('阻力位', 5), ('2', 6), ('：', 7), ('1.1240', 8), ('1', 9)]\n"
     ]
    }
   ],
   "source": [
    "word2id = {}\n",
    "for doc in copus_df[\"cut_text\"].values:\n",
    "    for word in doc.split():\n",
    "        if word not in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "print(len(word2id), list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['货币', ':', '欧元', '/', '美元', '阻力位', '2', '：', '1.1240', '阻力位']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copus_list = copus_df[\"cut_text\"].apply(lambda doc: doc.split()).values\n",
    "copus_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init ...\n",
      "nw:  [[18 13 17 ..., 16 19 22]\n",
      " [54 44 61 ..., 48 58 54]\n",
      " [38 41 38 ..., 39 39 46]\n",
      " ..., \n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  1  0  0]\n",
      " [ 0  0  0 ...,  1  0  0]] nd:  [[12  9  8 ..., 19 18  8]\n",
      " [10  9 16 ..., 14 11 11]\n",
      " [14 18 13 ..., 13 15 21]\n",
      " ..., \n",
      " [ 9  9  9 ...,  8  6  6]\n",
      " [59 45 65 ..., 60 52 59]\n",
      " [ 9 22 16 ..., 20 14 15]] nwsum:  [ 99885  99861 100278  99949  99913 100190 100057 100198  99980  99563\n",
      " 100002 100109  99876 100424 100275 100198 100052 100130  99809 100601] ndsum:  [ 273  199  303 ...,  162 1120  296]\n",
      "start iterating ...\n",
      "iterating # 0  ...\n",
      "iter time cost:  418.23634696006775\n",
      "nw:  [[21  6 14 ..., 13 16 36]\n",
      " [47 43 54 ..., 65 50 49]\n",
      " [41 36 11 ..., 51 40 42]\n",
      " ..., \n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  1 ...,  0  0  1]] nd:  [[14 16 10 ..., 17 20  0]\n",
      " [15  7 28 ..., 11  9  8]\n",
      " [13  6  8 ...,  5  5 25]\n",
      " ..., \n",
      " [15  5 20 ..., 10  0  8]\n",
      " [50 21 68 ..., 85 79 46]\n",
      " [ 7 30 11 ..., 29 13  8]] nwsum:  [ 98980 101148  99858  99436 100507 100792  99859 100052  99367  99737\n",
      " 101220 100199  99521 102078  99039  99902  99792 100282  99373 100208] ndsum:  [ 273  199  303 ...,  162 1120  296]\n",
      "[  23   48   42  101   28  280  172  283  285    7   59  119   61  513  383\n",
      "  507   46 2263  442   91]\n",
      "， 的 。 在 了 “ 、 ” 是 ： 和 月 也 他 年 有 将 我 中 为\n",
      "calculating theta and phi\n",
      "[  23   48   42  101  283  172   28  280  285   59    7   61  119  383  513\n",
      "  326 2263  133  507  988]\n",
      "， 的 。 在 ” 、 了 “ 是 和 ： 也 月 年 他 日 我 上 有 就\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101  28 283 280 172 285   7  59 513  61 383 119 507 577 177\n",
      " 442 326]\n",
      "， 的 。 在 了 ” “ 、 是 ： 和 他 也 年 月 有 》 对 中 日\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  283  172  280  285   59    7  513   61  119 2263\n",
      "  507  383   46  326  988]\n",
      "， 的 。 在 了 ” 、 “ 是 和 ： 他 也 月 我 有 年 将 日 就\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  280  283  172  285   59    7  119  513   61  442\n",
      "  326  507 2263  383  267]\n",
      "， 的 。 在 了 “ ” 、 是 和 ： 月 他 也 中 日 有 我 年 中国\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101 283  28 280 172 285  59   7  61 119 513 383  46 507  91\n",
      " 522 267]\n",
      "， 的 。 在 ” 了 “ 、 是 和 ： 也 月 他 年 将 有 为 都 中国\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101  28 280 172 283 285  59   7 513 119 383  61 507  46 326\n",
      " 133 177]\n",
      "， 的 。 在 了 “ 、 ” 是 和 ： 他 月 年 也 有 将 日 上 对\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  172  280  283  285   59    7   61  119  513  507\n",
      "  442  133   46 2263  988]\n",
      "， 的 。 在 了 、 “ ” 是 和 ： 也 月 他 有 中 上 将 我 就\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  280  283  172  285   59    7  513   61  119  507\n",
      "  326  383  442  133 2263]\n",
      "， 的 。 在 了 “ ” 、 是 和 ： 他 也 月 有 日 年 中 上 我\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  283  280  172  285   59    7   61  513  119  507\n",
      "   46  383 2263  463  326]\n",
      "， 的 。 在 了 ” “ 、 是 和 ： 也 他 月 有 将 年 我 与 日\n",
      "calculating theta and phi\n",
      "[  23   48   42  101  172   28  280  283  285    7   59  513   61  119   46\n",
      "  383 2263  326   91  566]\n",
      "， 的 。 在 、 了 “ ” 是 ： 和 他 也 月 将 年 我 日 为 《\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  283  280  285  172   59  513    7  119  383   61\n",
      "  507 2263  566   91  463]\n",
      "， 的 。 在 了 ” “ 是 、 和 他 ： 月 年 也 有 我 《 为 与\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101  28 283 172 285 280  59   7 119 513  61 326 383 507 177\n",
      " 442  46]\n",
      "， 的 。 在 了 ” 、 是 “ 和 ： 月 他 也 日 年 有 对 中 将\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  172  283  280  285   59    7  119   61  507  513\n",
      "  326  133  383  442 2263]\n",
      "， 的 。 在 了 、 ” “ 是 和 ： 月 也 有 他 日 上 年 中 我\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  280  283  172  285   59    7  119   61  513  507\n",
      "  133  383  522  326 2263]\n",
      "， 的 。 在 了 “ ” 、 是 和 ： 月 也 他 有 上 年 都 日 我\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101  28 172 280 283 285  59   7 119  61 513 133 507  46  91\n",
      " 463 326]\n",
      "， 的 。 在 了 、 “ ” 是 和 ： 月 也 他 上 有 将 为 与 日\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  172  283  280  285   59    7  513  119   61  507\n",
      " 2263  577  383   91  544]\n",
      "， 的 。 在 了 、 ” “ 是 和 ： 他 月 也 有 我 》 年 为 ）\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101  28 283 172 280 285  59   7 119  61 513 133 326 507  91\n",
      " 988 577]\n",
      "， 的 。 在 了 ” 、 “ 是 和 ： 月 也 他 上 日 有 为 就 》\n",
      "calculating theta and phi\n",
      "[ 23  48  42 101 280  28 283 172 285  59   7 513  61 119 507 383  91 442\n",
      "  46 133]\n",
      "， 的 。 在 “ 了 ” 、 是 和 ： 他 也 月 有 年 为 中 将 上\n",
      "calculating theta and phi\n",
      "[  23   48   42  101   28  280  283  172  285   59    7   61  119  513  383\n",
      "  507  326 2263  442  566]\n",
      "， 的 。 在 了 “ ” 、 是 和 ： 也 月 他 年 有 日 我 中 《\n",
      "calculating theta and phi\n",
      "iterating # 1  ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-2189f0848d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopus_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(\"z: \", z, \"theta: \", theta, \"phi: \", phi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time cost: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-d6ab3b1aa626>\u001b[0m in \u001b[0;36mlda_train\u001b[0;34m(doc_set, word2id, K, alpha, beta, iter_number, with_debug_log)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 存放各主题概率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m                            \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnwsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;31m# 重新采样\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mnew_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_discrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "z, theta, phi = lda_train(copus_list, word2id, 20)\n",
    "# print(\"z: \", z, \"theta: \", theta, \"phi: \", phi)\n",
    "print(\"time cost: \", time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (10, 106280))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape, phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict([(v, k) for k, v in word2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00353357  0.00353357  0.00353357  0.00353357  0.01766784  0.5335689\n",
      "  0.00353357  0.00353357  0.00353357  0.42402827] 5\n"
     ]
    }
   ],
   "source": [
    "print(theta[0], np.argmax(theta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_topic in phi:\n",
    "    print(one_topic.argsort()[: -21: -1])\n",
    "    print(\" \".join([id2word[i] for i in one_topic.argsort()[: -21: -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
