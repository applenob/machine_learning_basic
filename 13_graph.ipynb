{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图模型总结\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/machine_learning_basic/master/res/crf.png)\n",
    "\n",
    "## 1.图模型的引入\n",
    "\n",
    "首先总结一下图模型，所谓图模型，其实就是在统计建模的时候，结合图论的思想。\n",
    "\n",
    "图模型=概率论+图论\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/machine_learning_basic/master/res/graph.png)\n",
    "\n",
    "让我们先从**朴素贝叶斯**开始思考，随机变量y和所有的观测变量X有关，但每个观测变量对于y来说，又是独立的，也就是我们说的“naive”。\n",
    "\n",
    "这基本上是最简单的随机变量的关系了:$P(X|y)=p(x_1|y)\\cdot p(x_2|y)...\\cdot p(x_n|y)$。\n",
    "\n",
    "那我们可以从这里引申出什么呢？如果把所有的随机变量，都用图论中的节点表示，变量间的关系，由边表示，暂时先不考虑边的方向的问题，那么朴素贝叶斯就可以很直观地画成上面的第一幅图。\n",
    "\n",
    "再来回忆一下**最大熵**。最大熵的建模思想并不是来源于图论，但是看看最大熵模型的表达式：\n",
    "\n",
    "$$P_w(y|x)=\\frac{1}{Z_w(x)}exp\\bigl(\\begin{smallmatrix}\n",
    "\\sum_{i=1}^{n} w_i\\cdot f_i(x,y)\n",
    "\\end{smallmatrix}\\bigr)$$\n",
    "\n",
    "我们换一个思路去想：特征函数$f_i(x,y)$刻画的是变量$x$和$y$之间的关系，这跟朴素贝叶斯中的条件概率不同，**条件概率是单向的**，$p(y|x)!=p(x|y)$，而**特征函数是双向的，或者说是无向的**，$f(x,y)=f(y,x)$。因此为了区分这二者的区别我们把图模型分为有向图和无向图模型两种。\n",
    "\n",
    "下面给出二者更加规范的定义（来自：[An introduction to conditional random fields](https://link.zhihu.com/?target=http%3A//homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**无向图**：\n",
    "\n",
    "考虑一系列随机变量$Y$，$s∈1,2,...|Y|$。$y$是$Y$的分布。\n",
    "\n",
    "认为$y$的概率分布可以表示成一系列和$Y$有关的**因素（factor）**的乘积。这个因素的形式是：$\\Psi_a(y_a)$，$a∈1,2,...,A$。\n",
    "**加粗代表是向量。**\n",
    "\n",
    "$$p(\\mathbf{y})=\\frac{1}{Z}\\prod_{a=1}^{A}\\Psi_a(\\mathbf{y}_a)$$\n",
    "\n",
    "其中$Z$是归一化因子。\n",
    "\n",
    "例：\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/machine_learning_basic/master/res/dag.png)\n",
    "\n",
    "\n",
    "$$p(y_1,y_2,y_3)\\propto \\Psi_1(y_1,y_2) \\Psi_2(y_2,y_3) \\Psi_3(y_3,y_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**有向图**：\n",
    "$G$是一个DAG（有向无环图），$π(s)$是$Y_s$的下标，DAG的模型可以这么理解：联合概率分布等于每个节点在它们的父节点的条件下的条件概率的累乘，写成：\n",
    "\n",
    "$$p(\\mathbf{y})=\\prod^S_{s=1}p(y_s|\\mathbf{y}_{π(s)})$$\n",
    "\n",
    "例：\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/machine_learning_basic/master/res/ug.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
