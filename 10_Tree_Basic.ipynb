{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树基础\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [概览](#概览)\n",
    "- [基础知识](#基础知识)\n",
    "- [算法](#算法)\n",
    "- [1.ID3算法](#1.ID3算法)\n",
    "    - [ID3算法流程](#ID3算法流程)\n",
    "- [2.C4.5算法](#2.C4.5算法)\n",
    "- [3.CART算法](#3.CART算法)\n",
    "    - [基尼系数](#基尼系数)\n",
    "    - [剪枝](#剪枝)\n",
    "    - [CART算法流程](#CART算法流程)\n",
    "    - [算法总结](#算法总结)\n",
    "- [决策树实践](#决策树实践)\n",
    "- [决策树的可解释性](#决策树的可解释性)\n",
    "- [参考链接](#参考链接)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概览\n",
    "\n",
    "决策树是一种分类和回归的基本模型，可从三个角度来理解它，即：\n",
    "  \n",
    "  ● 一棵树。\n",
    "  \n",
    "  ● if-then规则的集合，该集合是决策树上的所有从根节点到叶节点的路径的集合。\n",
    "  \n",
    "  ● 定义在特征空间与类空间上的条件概率分布，决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。\n",
    "  \n",
    "  主要的优点有两个：\n",
    "  \n",
    "  ● 模型具有可解释性，容易向业务部门人员描述。\n",
    "  \n",
    "  ● 分类速度快。\n",
    "  \n",
    "\n",
    "## 基础知识\n",
    "\n",
    "**熵**：$H(x) = -\\sum_{i=1}^np_ilog(p_i)$\n",
    "\n",
    "**条件熵**：$H(Y|X) = H(X,Y)-H(X) = \\sum_XP(X)H(Y|X) = -\\sum_{X,Y}logP(Y|X)$\n",
    "\n",
    "**基尼系数（Gini index）**：$Gini(p) = \\sum_{k=1}^Kp_k(1-p_k) = 1-\\sum_{k=1}^Kp_k^2$，基尼指数反应了从数据集中随机抽取两个样本，其类标不一致的概率。\n",
    "\n",
    "## 算法\n",
    "\n",
    "决策树的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化。\n",
    "\n",
    "所以决策树的本质和其他机器学习模型是一致的，有一个损失函数，然后去优化这个函数；然而，区别就在于如何优化。\n",
    "\n",
    "决策树采用**启发式算法**来近似求解最优化问题，得到的是次最优的结果。\n",
    "\n",
    "该启发式算法可分为三步：\n",
    "\n",
    "- 特征选择\n",
    "- 模型生成\n",
    "- 决策树的剪枝\n",
    "\n",
    "决策树学习算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割。\n",
    "\n",
    "选择最优特征要根据**特征的分类能力**，特征分类能力的衡量通常采用信息增益或信息增益比。\n",
    "\n",
    "决策树学习常用的算法主要有以下三种：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.ID3算法\n",
    "\n",
    "ID3使用**信息增益**作为特征选取的依据：\n",
    "\n",
    "$G(D, A) = H(D) - H(D|A)$，即**经验熵**和**经验条件熵**的差值，其中$D$是训练数据集，$A$是特征。\n",
    "\n",
    "$H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}log\\frac{|C_k|}{|D|}$，其中，$|C_k|$是属于类$C_k$的个数，$|D|$是所有样本的个数。\n",
    "\n",
    "$H(D|A)=\\sum_{i=1}^np_{a_i}H(D|a_i)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_i|}log\\frac{|D_{ik}|}{|D_i|}$，其中，特征$A$有$n$个不同的取值$\\{a_1, a_2, ..., a_n\\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1, D_2, ..., D_n$， $|D_i|$是$D_i$的样本个数，$D_{ik}$是$D_i$中属于类$C_k$的样本集合。\n",
    "\n",
    "### ID3算法流程\n",
    "\n",
    "- 1.计算$A$中各个特征对$D$的信息增益，选择信息增益最大的特征：$A_g$。\n",
    "- 2.若$A_g$的信息增益小于**阈值$\\epsilon$**，则置为单结点树，并将$D$中实例数最多的类$C_k$作为该结点的类标记。\n",
    "- 3.否则，对$A_g$的每一可能值：$a_i$，依据$A_g = a_i$将$D$分割为若干非空子集$D_i$，同样，将$D_i$中实例数最多的类作为类标，构建子结点。\n",
    "- 4.对第$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用上面1-3步。\n",
    "\n",
    "## 2.C4.5算法\n",
    "\n",
    "C4.5使用**信息增益比**，作为特征选取的依据：\n",
    "\n",
    "**信息增益比**：$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$，即信息增益除以训练集$D$关于特征$A$的熵，$H_A(D) = -\\sum_{i=1}^n\\frac{D_i}{D}log_2\\frac{D_i}{D}$，$n$是特征$A$取值的个数。\n",
    "\n",
    "**为什么使用信息增益比？**先回顾信息增益：$H(D|A)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^{K}\\frac{|D_{ik}|}{|D_i|}log\\frac{|D_{ik}|}{|D_i|}$，对于极限情况，如果某个特征$A$可以将数据集$D$完全分隔开，且每个子集的个数都是1，那么$log\\frac{|D_{ik}|}{|D_i|} = log1 = 0$，于是信息增益取得最大。但这样的特征并不是最好的。\n",
    "\n",
    "也就是说，使用信息增益作为特征选择的标准时，容易偏向于那些**取值比较多**的特征，导致训练出来的树非常的**宽**然而**深度不深**的树，非常容易导致**过拟合**。\n",
    "\n",
    "而采用信息增益比则有效地抑制了这个缺点：取值多的特征，以它作为根节点的单节点树的熵很大，即$H_A(D)$较大，导致信息增益比减小，在特征选择上会更加合理。\n",
    "\n",
    "C4.5具体算法类似于ID3算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.CART算法\n",
    "\n",
    "**CART（Classification And Regression Tree）**本身是一种分类回归树，即，它既可以用来解决分类问题，也可以用来解决回归问题。\n",
    "\n",
    "**CART**树是一棵**二叉树**，内部结点特征的取值是“是”和“否”，左分支是取值为“是”的分支，右分支是取值是“否”的分支。\n",
    "\n",
    "因此，注意到CART的生成过程和前面的ID3和C4.5略有不同。\n",
    "\n",
    "### 基尼系数\n",
    "\n",
    "CART使用**基尼系数（Gini index）**最小化准则，进行特征选择。\n",
    "\n",
    "**基尼系数（Gini index）**：\n",
    "\n",
    "$Gini(D) = 1-\\sum_{k=1}^{K}(\\frac{|C_k|}{|D|})^2$\n",
    "\n",
    "$Gini(D,A) = \\sum_i\\frac{D_i}{D}Gini(D_i)$\n",
    "\n",
    "基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的**不纯度（impurity）**，基尼指数越大，纯度越低，和熵类似。\n",
    "\n",
    "### 剪枝\n",
    "\n",
    "CART算法算法涉及到决策树的**剪枝**：即，为了防止出现过拟合现象，要把过于复杂的树进行剪枝，将其简化。\n",
    "\n",
    "决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或者代价函数（cost function）来实现。\n",
    "\n",
    "**损失函数**：$C_α(T) = C(T)+α|T|=\\sum_{t=1}^{|T|}N_tH_t(T)+α|T|$\n",
    "\n",
    "其中，$|T|$是树$T$的叶节点个数，$t$是其中一个结点，$N_t$是这个结点的样本个数，$H_t(T)$是这个结点的经验熵。\n",
    "\n",
    "$C(T)$表示模型对训练数据的预测误差， $α|T|$则是正则化项。\n",
    "\n",
    "使用叶子结点的熵作为的模型的评价是因为：\n",
    "\n",
    "**如果分到该叶节点的所有样本都属于同一类，那么分类效果最好，熵最小。**\n",
    "\n",
    "**一般的剪枝算法**：\n",
    "\n",
    "1.计算每个结点的经验熵。\n",
    "\n",
    "2.递归地从叶节点向上回缩：设一叶结点回缩到父结点之前和之后，树分别是$T_B$和$T_A$，其对应的损失函数值分别是$C_α(T_B)$与$C_α(T_A)$，如果$C_α(T_A)≤C_α(T_B)$，则剪枝，即将父节点变成新的叶结点。\n",
    "\n",
    "### CART算法流程\n",
    "\n",
    "**1.决策树生成**：\n",
    "\n",
    "- （1）. 对现有特征A的每一个特征，每一个可能的取值a，**根据样本点对$A=a$的测试是“是”还是“否”**，将$D$分割成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数。\n",
    "- （2）.选择基尼指数最小的特征机器对应的切分点作为**最优特征**和**最优切分点**。\n",
    "- （3）.递归调用，直到满足停止条件。\n",
    "\n",
    "**停止条件**：\n",
    "\n",
    "- 结点中样本个数小于预定阈值；\n",
    "- 样本集的基尼指数小于预定阈值（基本属于同一类）；\n",
    "- 没有更多特征。\n",
    "    \n",
    "**2.CART剪枝**：\n",
    "\n",
    "相比一般剪枝算法，CART剪枝算法的优势在于，**不用提前确定$α$值**，而是在剪枝的同时找到最优的α值。\n",
    "\n",
    "对于固定的$α$值，一定存在让$C_α(T)$最小的子树，记为$T_α$。\n",
    "\n",
    "对于某个结点$t$，单结点树的损失函数是：$C_α(t) = C(t) + α$，而以$t$为根的子树$T_t$的损失函数是：$C_α(T_t) = C(T_t) + α|T_t|$。\n",
    "\n",
    "当$α$充分小的时候，有$C_α(T_t) < C_α(t)$；\n",
    "\n",
    "当$α$增大到某一$α$时有：$C_α(T_t) = C_α(t)$。\n",
    "\n",
    "即，只要$α = \\frac{ C(t)-C(T_t)}{|T_t|-1}$，就可以保证$T_t$和$t$有相同的损失函数，也就代表着可以对$T_t$剪枝。\n",
    "\n",
    "因此，对于每个内部结点，计算$g(t) = \\frac{C(t)-C(T_t)}{|T_t|-1}$，代表**剪枝后整体损失函数减少的程度。**\n",
    "\n",
    "将最小的$g(t)$设为$α_1$，剪枝得$T_1$，不断地重复此步骤，可以增加$α$，获得一系列$T$。\n",
    "\n",
    "通过**交叉验证**，从剪枝得到的字数序列$T_0, T_1, ..., T_n$中选取最优子树$T_α$。\n",
    "\n",
    "## 算法总结\n",
    "\n",
    "ID3算法/C4.5算法/CART算法。\n",
    "\n",
    "ID3算法和C4.5算法用于生成**分类树**，区别主要在于选取特征的依据，前者是**信息增益**，后者是**信息增益比**。\n",
    "\n",
    "CART算法可以生成**分类树**和**回归树**，使用**基尼指数**选取特征，并且不用提前确定$α$值，而是在剪枝的同时找到最优的α值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 决策树实践\n",
    "\n",
    "使用sklearn的决策树实现来看看实践中如何使用决策树模型，\n",
    "\n",
    "sklearn中的决策树模型：**DecisionTreeClassifier**。\n",
    "\n",
    "```\n",
    "class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "```\n",
    "\n",
    "重要参数：\n",
    "\n",
    "- `criterion`: “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "- `max_depth`: 树的最大深度。\n",
    "- `min_impurity_decrease`: 最小的基尼指数下降。\n",
    "\n",
    "下面代码摘自[这里](http://blog.csdn.net/sinat_22594309/article/details/59090895)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确率是95.56%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn import datasets  \n",
    "  \n",
    "#读取数据，划分训练集和测试集  \n",
    "iris=datasets.load_iris()  \n",
    "# 只保留数据集的前五个特征\n",
    "x=iris.data[:, :5]\n",
    "y=iris.target  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=1)  \n",
    "#模型训练  \n",
    "model = DecisionTreeClassifier(max_depth=3)  \n",
    "model = model.fit(x_train,y_train)  \n",
    "y_test_hat = model.predict(x_test)  \n",
    "res=y_test == y_test_hat  \n",
    "acc=np.mean(res)  \n",
    "print '正确率是%.2f%%'%(acc*100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较不同深度对预测准确率的影响："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确率是55.56%\n",
      "正确率是55.56%\n",
      "正确率是75.56%\n",
      "正确率是75.56%\n",
      "正确率是75.56%\n",
      "正确率是75.56%\n",
      "正确率是66.67%\n",
      "正确率是64.44%\n",
      "正确率是62.22%\n",
      "正确率是57.78%\n",
      "正确率是62.22%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEKCAYAAADzQPVvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkSkCfx/FF33f1qUglsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6fe7c7f810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline \n",
    "#模型训练  \n",
    "depth_test=np.linspace(1,10,11)  \n",
    "accurate=[]  \n",
    "for depth in depth_test:  \n",
    "    test_model=DecisionTreeClassifier(max_depth=depth)  \n",
    "    test_model=test_model.fit(x_train,y_train)  \n",
    "    y_test_hat=test_model.predict(x_test)  \n",
    "    res=y_test==y_test_hat  \n",
    "    acc=np.mean(res)  \n",
    "    accurate.append(acc)  \n",
    "    print '正确率是%.2f%%'%(acc*100)  \n",
    "plt.plot(depth_test,accurate,'r-')  \n",
    "plt.grid()  \n",
    "plt.title('Accuracy by tree_depth')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树的可解释性\n",
    "\n",
    "本文一开始提到决策树的一个优点是其可解释性。\n",
    "\n",
    "接下来通过一些代码来演示其可解释性，代码来自sklearn官网。\n",
    "\n",
    "1.**用graphviz可视化决策树**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "import sklearn.tree  as tree\n",
    "# sklearn 支持将决策树模型导出成可视化的graphviz\n",
    "dot_data = tree.export_graphviz(model, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "# graph.render(\"iris\") \n",
    "#graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.**手动输出决策树信息**\n",
    "\n",
    "sklearn中决策树模型的信息保存在`xxx.tree_`中：\n",
    "\n",
    "```\n",
    "Attributes\n",
    "----------\n",
    "node_count : int\n",
    "    The number of nodes (internal nodes + leaves) in the tree.\n",
    "\n",
    "capacity : int\n",
    "    The current capacity (i.e., size) of the arrays, which is at least as\n",
    "    great as `node_count`.\n",
    "\n",
    "max_depth : int\n",
    "    The maximal depth of the tree.\n",
    "\n",
    "children_left : array of int, shape [node_count]\n",
    "    children_left[i] holds the node id of the left child of node i.\n",
    "    For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
    "    children_left[i] > i. This child handles the case where\n",
    "    X[:, feature[i]] <= threshold[i].\n",
    "\n",
    "children_right : array of int, shape [node_count]\n",
    "    children_right[i] holds the node id of the right child of node i.\n",
    "    For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
    "    children_right[i] > i. This child handles the case where\n",
    "    X[:, feature[i]] > threshold[i].\n",
    "\n",
    "feature : array of int, shape [node_count]\n",
    "    feature[i] holds the feature to split on, for the internal node i.\n",
    "\n",
    "threshold : array of double, shape [node_count]\n",
    "    threshold[i] holds the threshold for the internal node i.\n",
    "\n",
    "value : array of double, shape [node_count, n_outputs, max_n_classes]\n",
    "    Contains the constant prediction value of each node.\n",
    "\n",
    "impurity : array of double, shape [node_count]\n",
    "    impurity[i] holds the impurity (i.e., the value of the splitting\n",
    "    criterion) at node i.\n",
    "\n",
    "n_node_samples : array of int, shape [node_count]\n",
    "    n_node_samples[i] holds the number of training samples reaching node i.\n",
    "\n",
    "weighted_n_node_samples : array of int, shape [node_count]\n",
    "    weighted_n_node_samples[i] holds the weighted number of training samples\n",
    "    reaching node i.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes:   13\n",
      "children_left:   [ 1  2  3 -1 -1 -1  7  8 -1 -1 11 -1 -1]\n",
      "children_right:  [ 6  5  4 -1 -1 -1 10  9 -1 -1 12 -1 -1]\n",
      "feature:  [ 0  1  0 -2 -2 -2  0  1 -2 -2  1 -2 -2]\n",
      "threshold:  [ 5.44999981  2.80000019  4.69999981 -2.         -2.         -2.          6.25\n",
      "  3.45000005 -2.         -2.          2.54999995 -2.         -2.        ]\n"
     ]
    }
   ],
   "source": [
    "# The decision estimator has an attribute called tree_  which stores the entire\n",
    "# tree structure and allows access to low level attributes. The binary tree\n",
    "# tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "# array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "# Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "# case the values of nodes of the other type are arbitrary!\n",
    "#\n",
    "# Among those arrays, we have:\n",
    "#   - left_child, id of the left child of the node\n",
    "#   - right_child, id of the right child of the node\n",
    "#   - feature, feature used for splitting the node\n",
    "#   - threshold, threshold value at the node\n",
    "#\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = model.tree_.node_count\n",
    "children_left = model.tree_.children_left\n",
    "children_right = model.tree_.children_right\n",
    "feature = model.tree_.feature\n",
    "threshold = model.tree_.threshold\n",
    "\n",
    "print \"n_nodes:  \", n_nodes\n",
    "print \"children_left:  \", children_left\n",
    "print \"children_right: \", children_right\n",
    "print \"feature: \", feature\n",
    "print \"threshold: \", threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到上面出现了-1和-2这些让人觉得奇怪的值，解释一下：\n",
    "```python\n",
    "TREE_LEAF = -1\n",
    "TREE_UNDEFINED = -2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 13 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 0] <= 5.44999980927 else to node 6.\n",
      "\tnode=1 test node: go to node 2 if X[:, 1] <= 2.80000019073 else to node 5.\n",
      "\t\tnode=2 test node: go to node 3 if X[:, 0] <= 4.69999980927 else to node 4.\n",
      "\t\t\tnode=3 leaf node.\n",
      "\t\t\tnode=4 leaf node.\n",
      "\t\tnode=5 leaf node.\n",
      "\tnode=6 test node: go to node 7 if X[:, 0] <= 6.25 else to node 10.\n",
      "\t\tnode=7 test node: go to node 8 if X[:, 1] <= 3.45000004768 else to node 9.\n",
      "\t\t\tnode=8 leaf node.\n",
      "\t\t\tnode=9 leaf node.\n",
      "\t\tnode=10 test node: go to node 11 if X[:, 1] <= 2.54999995232 else to node 12.\n",
      "\t\t\tnode=11 leaf node.\n",
      "\t\t\tnode=12 leaf node.\n"
     ]
    }
   ],
   "source": [
    "# 遍历树，获取每个结点的深度和每个结点是否是叶结点\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules used to predict sample 0: \n",
      "decision id node 9 : (X_test[0, -2] (= 5.8) > -2.0)\n",
      "\n",
      "The following samples [0, 1] share the node [0] in the tree\n",
      "It is 7 % of all nodes.\n"
     ]
    }
   ],
   "source": [
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "\n",
    "node_indicator = model.decision_path(x_test)\n",
    "\n",
    "# Similarly, we can also have the leaves ids reached by each sample.\n",
    "\n",
    "leave_id = model.apply(x_test)\n",
    "\n",
    "# Now, it's possible to get the tests that were used to predict a sample or\n",
    "# a group of samples. First, let's make it for the sample.\n",
    "\n",
    "sample_id = 0\n",
    "node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                    node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    if leave_id[sample_id] != node_id:\n",
    "        continue\n",
    "\n",
    "    if (x_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "          % (node_id,\n",
    "             sample_id,\n",
    "             feature[node_id],\n",
    "             x_test[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))\n",
    "\n",
    "# For a group of samples, we have the following common node.\n",
    "sample_ids = [0, 1]\n",
    "common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n",
    "                len(sample_ids))\n",
    "\n",
    "common_node_id = np.arange(n_nodes)[common_nodes]\n",
    "\n",
    "print(\"\\nThe following samples %s share the node %s in the tree\"\n",
    "      % (sample_ids, common_node_id))\n",
    "print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 参考链接\n",
    " \n",
    "-  [数据挖掘面试题之决策树必知必会](http://www.jianshu.com/p/fb97b21aeb1d)\n",
    "- [机器学习笔记（五）决策树算法及实践](http://blog.csdn.net/sinat_22594309/article/details/59090895)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
